# -*- coding: utf-8 -*-
"""dip skip standalone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ucgyM1k1LP6WKHKmZMZJjt4mNR-N0Fnq
"""

import torch
from matplotlib import pyplot as plt
import torchvision
plt.rcParams['axes.grid']=False
import numpy as np
import itertools
device = 'cuda' if torch.cuda.is_available() else 'cpu'
tensor_to_numpy = lambda t: t.detach().cpu().numpy()
tensor_to_im =  lambda t:tensor_to_numpy(t.permute(0,2,3,1))[0]
import os
import shutil
import skimage.transform

def encblock(module_list,inchan,nchan,k,pad='reflection'):
  
  if pad == 'reflection':
    to_pad = int((k - 1) / 2)
    padding = torch.nn.ReflectionPad2d(to_pad)
    to_pad = 0
    pass
  else:
    to_pad = int((k-1)//2)
    pass
  

  block = \
  [padding if pad == 'reflection' else None,
   torch.nn.Conv2d(inchan,nchan,k,stride = 2,padding=to_pad),
   torch.nn.BatchNorm2d(nchan),
   torch.nn.LeakyReLU(),
   
   padding if pad == 'reflection' else None,
   torch.nn.Conv2d(nchan,nchan,k,stride = 1,padding=to_pad),
   torch.nn.BatchNorm2d(nchan),
   torch.nn.LeakyReLU(),   
  ]
  
  block = filter(lambda x:x is not None,block)
  module_list.extend(block)
  return nchan

def decblock(module_list,inchan,nchan,k,pad='reflection'):
  
  if pad == 'reflection':
    to_pad = int((k - 1) / 2)
    padding = torch.nn.ReflectionPad2d(to_pad)
    to_pad = 0
    pass
  else:
    to_pad = int((k-1)//2)

  block = \
  [torch.nn.BatchNorm2d(inchan),
   padding if pad == 'reflection' else None,
   torch.nn.Conv2d(inchan,nchan,k,stride = 1,padding=to_pad),
   torch.nn.LeakyReLU(),
   torch.nn.Conv2d(nchan,nchan,1,stride = 1),
   torch.nn.BatchNorm2d(nchan),
   torch.nn.LeakyReLU(),   
  ]
  
  block = filter(lambda x:x is not None,block)
  module_list.extend(block)
  return nchan

def skipblock(module_list,inchan,nchan,k,pad='reflection'):
    if pad == 'reflection':
        to_pad = int((k - 1) / 2)
        padding = torch.nn.ReflectionPad2d(to_pad)
        to_pad = 0

    else:
        to_pad = int((k-1)//2)
    
        
    block = \
    [
    padding if pad == 'reflection' else None,
    torch.nn.Conv2d(inchan,nchan,k,stride = 1,padding=to_pad),
    torch.nn.BatchNorm2d(nchan),
    torch.nn.LeakyReLU(),
    ] 

    block = filter(lambda x:x is not None,block)
    module_list.extend(block)
    return nchan

def get_block_params(block):
    params = []
    for b in block:
        if b.__class__ in [torch.nn.Conv2d,torch.nn.BatchNorm2d]:
            params.extend(list(b.parameters()))
    return params

def to_cuda(block):
    if block.__class__ == list:
        for i,b in enumerate(block):
            block[i] = b.cuda()

def block_forward(block,x,feats,feat_names):
    for b in block:
        x = b(x)
        feats.extend([x])
        b_name = str(b.__class__).split('.')[-1]
        feat_names.extend([b_name])
        pass
    return x
    pass

class dip(torch.nn.Module):
    def __init__(self):
        super(dip,self).__init__()
        pass
    def make(self,
             inchan,
            nchan_list,
            kernels,
            skip_nchan_list,
             skip_kernels,
             make_enc = True,
             pad='reflection'
            ):
        
        upsample = []

        operations = {}
        
        encblocks = []
        operations['encblocks']=encblocks
        encblock_params = []
        
        skipblocks = []
        operations['skipblocks']=skipblocks
        skipblock_params = []
        
        has_skip = len(skip_nchan_list) > 0
        
        if make_enc:
            for l,(nchan,k) in enumerate(zip(nchan_list[::-1],kernels)):
                encblock_l = []            
                inchan = encblock(encblock_l,inchan,nchan,k)
                to_cuda(encblock_l)
                encblocks.append(encblock_l)
                encblock_params.append(get_block_params(encblock_l))
                if has_skip:
                    skipblock_l = []
                    inchan_skip = skipblock(skipblock_l,inchan,skip_nchan_list[l],skip_kernels[l])  
                    to_cuda(skipblock_l)
                    skipblocks.append(skipblock_l)
                    skipblock_params.append(get_block_params(skipblock_l))
                pass
            
            upsample = [torch.nn.Upsample(scale_factor = 2,mode = 'bilinear')]


        decblocks = []
        operations['decblocks']=decblocks
        decblock_params = []
        
        for l,(nchan,k) in enumerate(zip(nchan_list,kernels)):  
            decblock_l  = []
            decblock_l.extend(upsample)
            if has_skip:
                inchan = inchan + skip_nchan_list[::-1][l]
            inchan = decblock(decblock_l,inchan,nchan,k)
            to_cuda(decblock_l)
            upsample = [torch.nn.Upsample(scale_factor = 2,mode = 'bilinear')]
            inchan = nchan
            decblocks.append(decblock_l)
            decblock_params.append(get_block_params(decblock_l))
            pass
#         import pdb;pdb.set_trace()
        operations['end']=[torch.nn.Conv2d(inchan,3,1,stride = 1),
                          torch.nn.Sigmoid(),
                         ]
        endblock_params = [list(operations['end'][0].parameters())]
        
        to_cuda(operations['end']) 

        self.operations = operations
        self.params = {'encblock':encblock_params,
                       'skipblock':skipblock_params,
                       'decblock': decblock_params,
                       'endblock':endblock_params,
                      }
    
    def forward(self,x):
        feats = []
        feat_names = []
        has_skip = len(self.operations['skipblocks']) > 0
        
        skip_xs = []
        
        for i,d in enumerate(self.operations['encblocks']):

            x = block_forward(d,x,feats,feat_names)
            
            if has_skip:
                s = self.operations['skipblocks'][i]
                skip_x = block_forward(s,x,[],[])
                skip_xs.append(skip_x)
        
        reverse_skip_xs = skip_xs[::-1]
                
        for i,u in enumerate(self.operations['decblocks']):
            if has_skip:
                skip_x = reverse_skip_xs[i]
                x = torch.cat([x,skip_x],1)
            x= block_forward(u,x,feats,feat_names)
            
            


        x = block_forward(self.operations['end'],x,feats,feat_names)

        return x
        pass

